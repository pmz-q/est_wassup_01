{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((39609, 34), (39609,), (10963, 34))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "ROOT_PATH = '../../data/01/'\n",
    "\n",
    "train_df = pd.read_csv(f'{ROOT_PATH}train.csv',index_col='ID')\n",
    "test_df = pd.read_csv(f'{ROOT_PATH}test.csv',index_col='ID')\n",
    "\n",
    "train_df = train_df[['사고일시','요일', '기상상태', '도로형태', '노면상태', '사고유형','ECLO']]\n",
    "test_df = test_df[['사고일시','요일', '기상상태', '도로형태', '노면상태', '사고유형']]\n",
    "\n",
    "train_df['사고일시'] = pd.to_datetime(train_df['사고일시'], format='%Y-%m-%d %H', errors='raise')\n",
    "train_df['시간'] = train_df['사고일시'].dt.hour\n",
    "train_df['월'] = train_df['사고일시'].dt.month\n",
    "test_df['사고일시'] = pd.to_datetime(test_df['사고일시'], format='%Y-%m-%d %H', errors='raise')\n",
    "test_df['시간'] = test_df['사고일시'].dt.hour\n",
    "test_df['월'] = test_df['사고일시'].dt.month\n",
    "train_df.drop(columns='사고일시',inplace=True)\n",
    "test_df.drop(columns='사고일시',inplace=True)\n",
    "\n",
    "train_df = pd.get_dummies(train_df)\n",
    "test_df = pd.get_dummies(test_df)\n",
    "\n",
    "X_trn = train_df.drop(columns='ECLO').astype(np.float32)\n",
    "X_trn.drop(columns='기상상태_안개',inplace=True)\n",
    "y_trn = train_df['ECLO'].astype(np.float32)\n",
    "\n",
    "X_tst = test_df.astype(np.float32)\n",
    "\n",
    "X_trn.shape, y_trn.shape, X_tst.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torchmetrics\n",
    "from typing import Optional, List\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "  \n",
    "from nn import ANN\n",
    "from torch.utils.data import TensorDataset\n",
    "# from utils import CustomDataset\n",
    "from torchmetrics import MeanAbsoluteError, MeanSquaredError, MeanSquaredLogError\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def train_one_epoch(\n",
    "    model: nn.Module,\n",
    "    criterion: callable,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    data_loader: DataLoader,\n",
    "    device: str\n",
    ") -> float:\n",
    "    '''train one epoch\n",
    "\n",
    "    Args:\n",
    "        model: model\n",
    "        criterion: loss\n",
    "        optimizer: optimizer\n",
    "        data_loader: data loader\n",
    "        device: device\n",
    "    '''\n",
    "    model.train()\n",
    "    total_loss = 0.\n",
    "    for X, y in data_loader:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        output = model(X)\n",
    "        loss = criterion(output, y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * len(y)\n",
    "    return total_loss / len(data_loader.dataset)\n",
    "\n",
    "def evaluate(\n",
    "    model: nn.Module,\n",
    "    criterion: callable,\n",
    "    data_loader: DataLoader,\n",
    "    device: str,\n",
    "    metric: Optional[torchmetrics.metric.Metric] = None,\n",
    "    multi_metrics: List[torchmetrics.metric.Metric] = None\n",
    ") -> float:\n",
    "    '''evaluate\n",
    "\n",
    "    Args:\n",
    "        model: model\n",
    "        criterions: list of criterion functions\n",
    "        data_loader: data loader\n",
    "        device: device\n",
    "    '''\n",
    "    model.eval()\n",
    "    total_loss = 0.\n",
    "\n",
    "    mae, mse, msle = (\n",
    "        MeanAbsoluteError().to(device),\n",
    "        MeanSquaredError().to(device),\n",
    "        MeanSquaredLogError().to(device)\n",
    "    )\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in data_loader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            output = model(X)\n",
    "            total_loss += criterion(output, y).item() * len(y)\n",
    "\n",
    "            if metric is not None:\n",
    "                metric.update(output, y)\n",
    "\n",
    "            if multi_metrics is not None:\n",
    "                for metric in multi_metrics:\n",
    "                    metric.update(output, y)\n",
    "\n",
    "    if isinstance(total_loss, torch.Tensor):\n",
    "        return total_loss.item() / len(data_loader.dataset)\n",
    "    else:\n",
    "        return total_loss / len(data_loader.dataset)\n",
    "\n",
    "def kfold_cross_validation(model: nn.Module, criterion:callable, device:str, X_trn:np.array, y_trn:np.array, n_splits:int=5):\n",
    "  from sklearn.model_selection import KFold\n",
    "  from torchmetrics import MeanAbsoluteError, MeanSquaredError, MeanSquaredLogError\n",
    "  # from sklearn.metrics import mean_absolute_error,mean_squared_error,mean_squared_log_error\n",
    "  from copy import deepcopy\n",
    "  \n",
    "  Kf = KFold(n_splits=n_splits, shuffle=True, random_state=2023)\n",
    "  nets = [deepcopy(model) for i in range(n_splits)]\n",
    "  scores = {\n",
    "  'mae': [],\n",
    "  'mse': [],\n",
    "  'msle': []\n",
    "  }\n",
    "  \n",
    "  for i, (trn_idx, val_idx) in enumerate(Kf.split(X_trn, y_trn)):\n",
    "    X, y = (\n",
    "        torch.tensor(X_trn.iloc[trn_idx].values).to(device),\n",
    "        torch.tensor(y_trn.iloc[trn_idx].values).to(device)\n",
    "    )\n",
    "    X_val, y_val = (\n",
    "        torch.tensor(X_trn.iloc[val_idx].values).to(device),\n",
    "        torch.tensor(y_trn.iloc[val_idx].values).to(device)\n",
    "    )\n",
    "    ds = TensorDataset(X, y)\n",
    "    ds_val = TensorDataset(X_val, y_val)\n",
    "    dl = DataLoader(ds, batch_size=32, shuffle=True)\n",
    "    dl_val = DataLoader(ds_val, batch_size=len(ds_val), shuffle=False)\n",
    "\n",
    "    net = nets[i]()\n",
    "    net.to(device)  # 모델을 디바이스로 이동\n",
    "\n",
    "    pbar = tqdm(range(50))\n",
    "    for j in pbar:\n",
    "        mae, mse, msle = (\n",
    "            MeanAbsoluteError().to(device),\n",
    "            MeanSquaredError().to(device),\n",
    "            MeanSquaredLogError().to(device)\n",
    "        )\n",
    "        criterion = nn.MSELoss(reduction='mean')\n",
    "        optimizer = torch.optim.Adam(net.parameters(), lr=0.0001)\n",
    "        loss = train_one_epoch(net, criterion, optimizer, dl, device)\n",
    "        loss_val = evaluate(net, criterion, dl_val, device, multi_metrics=[mae, mse, msle])\n",
    "        mae, mse, msle = mae.compute(), mse.compute(), msle.compute()\n",
    "        pbar.set_postfix(trn_loss=loss, val_loss=loss_val)\n",
    "        print(f'Epoch {j+1}/{300} - Training Loss: {loss:.4f}, Validation Loss: {loss_val:.4f}')\n",
    "\n",
    "    scores[\"mae\"].append(mae.item())\n",
    "    scores[\"mse\"].append(mse.item())\n",
    "    scores[\"msle\"].append(msle.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sdk/miniconda3/envs/MathAI/lib/python3.10/site-packages/torch/cuda/__init__.py:138: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 11040). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "238d210b397a4f1b8df379347771bccf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300 - Training Loss: 18.6309, Validation Loss: 13.2637\n",
      "Epoch 2/300 - Training Loss: 10.7179, Validation Loss: 10.8745\n",
      "Epoch 3/300 - Training Loss: 10.1541, Validation Loss: 10.8367\n",
      "Epoch 4/300 - Training Loss: 10.1495, Validation Loss: 10.8365\n",
      "Epoch 5/300 - Training Loss: 10.1500, Validation Loss: 10.8361\n",
      "Epoch 6/300 - Training Loss: 10.1493, Validation Loss: 10.8343\n",
      "Epoch 7/300 - Training Loss: 10.1486, Validation Loss: 10.8316\n",
      "Epoch 8/300 - Training Loss: 10.1402, Validation Loss: 10.8205\n",
      "Epoch 9/300 - Training Loss: 10.1237, Validation Loss: 10.8045\n",
      "Epoch 10/300 - Training Loss: 10.1011, Validation Loss: 10.7752\n",
      "Epoch 11/300 - Training Loss: 10.0684, Validation Loss: 10.7359\n",
      "Epoch 12/300 - Training Loss: 10.0265, Validation Loss: 10.6910\n",
      "Epoch 13/300 - Training Loss: 9.9806, Validation Loss: 10.6496\n",
      "Epoch 14/300 - Training Loss: 9.9449, Validation Loss: 10.6236\n",
      "Epoch 15/300 - Training Loss: 9.9248, Validation Loss: 10.6080\n",
      "Epoch 16/300 - Training Loss: 9.9111, Validation Loss: 10.6228\n",
      "Epoch 17/300 - Training Loss: 9.9038, Validation Loss: 10.5965\n",
      "Epoch 18/300 - Training Loss: 9.9014, Validation Loss: 10.5918\n",
      "Epoch 19/300 - Training Loss: 9.8957, Validation Loss: 10.5932\n",
      "Epoch 20/300 - Training Loss: 9.8951, Validation Loss: 10.5875\n",
      "Epoch 21/300 - Training Loss: 9.8902, Validation Loss: 10.5869\n",
      "Epoch 22/300 - Training Loss: 9.8869, Validation Loss: 10.5868\n",
      "Epoch 23/300 - Training Loss: 9.8850, Validation Loss: 10.5805\n",
      "Epoch 24/300 - Training Loss: 9.8837, Validation Loss: 10.5889\n",
      "Epoch 25/300 - Training Loss: 9.8799, Validation Loss: 10.5974\n",
      "Epoch 26/300 - Training Loss: 9.8767, Validation Loss: 10.5734\n",
      "Epoch 27/300 - Training Loss: 9.8792, Validation Loss: 10.5730\n",
      "Epoch 28/300 - Training Loss: 9.8733, Validation Loss: 10.5880\n",
      "Epoch 29/300 - Training Loss: 9.8726, Validation Loss: 10.5651\n",
      "Epoch 30/300 - Training Loss: 9.8703, Validation Loss: 10.5632\n",
      "Epoch 31/300 - Training Loss: 9.8710, Validation Loss: 10.5664\n",
      "Epoch 32/300 - Training Loss: 9.8706, Validation Loss: 10.6046\n",
      "Epoch 33/300 - Training Loss: 9.8686, Validation Loss: 10.5696\n",
      "Epoch 34/300 - Training Loss: 9.8657, Validation Loss: 10.5592\n",
      "Epoch 35/300 - Training Loss: 9.8682, Validation Loss: 10.5672\n",
      "Epoch 36/300 - Training Loss: 9.8674, Validation Loss: 10.5689\n",
      "Epoch 37/300 - Training Loss: 9.8644, Validation Loss: 10.5561\n",
      "Epoch 38/300 - Training Loss: 9.8649, Validation Loss: 10.5601\n",
      "Epoch 39/300 - Training Loss: 9.8646, Validation Loss: 10.5624\n",
      "Epoch 40/300 - Training Loss: 9.8629, Validation Loss: 10.5532\n",
      "Epoch 41/300 - Training Loss: 9.8612, Validation Loss: 10.5483\n",
      "Epoch 42/300 - Training Loss: 9.8615, Validation Loss: 10.5560\n",
      "Epoch 43/300 - Training Loss: 9.8607, Validation Loss: 10.5464\n",
      "Epoch 44/300 - Training Loss: 9.8581, Validation Loss: 10.5739\n",
      "Epoch 45/300 - Training Loss: 9.8600, Validation Loss: 10.5446\n",
      "Epoch 46/300 - Training Loss: 9.8598, Validation Loss: 10.5445\n",
      "Epoch 47/300 - Training Loss: 9.8559, Validation Loss: 10.5440\n",
      "Epoch 48/300 - Training Loss: 9.8579, Validation Loss: 10.5435\n",
      "Epoch 49/300 - Training Loss: 9.8538, Validation Loss: 10.5434\n",
      "Epoch 50/300 - Training Loss: 9.8576, Validation Loss: 10.5473\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d35d519f38014a668dd4000139bb590c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300 - Training Loss: 17.2843, Validation Loss: 11.0937\n",
      "Epoch 2/300 - Training Loss: 10.8522, Validation Loss: 9.4248\n",
      "Epoch 3/300 - Training Loss: 10.5032, Validation Loss: 9.4246\n",
      "Epoch 4/300 - Training Loss: 10.5015, Validation Loss: 9.4252\n",
      "Epoch 5/300 - Training Loss: 10.5023, Validation Loss: 9.4248\n",
      "Epoch 6/300 - Training Loss: 10.5020, Validation Loss: 9.4224\n",
      "Epoch 7/300 - Training Loss: 10.4978, Validation Loss: 9.4200\n",
      "Epoch 8/300 - Training Loss: 10.4877, Validation Loss: 9.4007\n",
      "Epoch 9/300 - Training Loss: 10.4682, Validation Loss: 9.3757\n",
      "Epoch 10/300 - Training Loss: 10.4439, Validation Loss: 9.3403\n",
      "Epoch 11/300 - Training Loss: 10.4118, Validation Loss: 9.2972\n",
      "Epoch 12/300 - Training Loss: 10.3709, Validation Loss: 9.2475\n",
      "Epoch 13/300 - Training Loss: 10.3336, Validation Loss: 9.2077\n",
      "Epoch 14/300 - Training Loss: 10.3044, Validation Loss: 9.1793\n",
      "Epoch 15/300 - Training Loss: 10.2878, Validation Loss: 9.1665\n",
      "Epoch 16/300 - Training Loss: 10.2777, Validation Loss: 9.1695\n",
      "Epoch 17/300 - Training Loss: 10.2683, Validation Loss: 9.1348\n",
      "Epoch 18/300 - Training Loss: 10.2639, Validation Loss: 9.1499\n",
      "Epoch 19/300 - Training Loss: 10.2625, Validation Loss: 9.1239\n",
      "Epoch 20/300 - Training Loss: 10.2557, Validation Loss: 9.1379\n",
      "Epoch 21/300 - Training Loss: 10.2519, Validation Loss: 9.1265\n",
      "Epoch 22/300 - Training Loss: 10.2507, Validation Loss: 9.1139\n",
      "Epoch 23/300 - Training Loss: 10.2464, Validation Loss: 9.1140\n",
      "Epoch 24/300 - Training Loss: 10.2455, Validation Loss: 9.1101\n",
      "Epoch 25/300 - Training Loss: 10.2438, Validation Loss: 9.1080\n",
      "Epoch 26/300 - Training Loss: 10.2410, Validation Loss: 9.1068\n",
      "Epoch 27/300 - Training Loss: 10.2390, Validation Loss: 9.1063\n",
      "Epoch 28/300 - Training Loss: 10.2387, Validation Loss: 9.1040\n",
      "Epoch 29/300 - Training Loss: 10.2353, Validation Loss: 9.1087\n",
      "Epoch 30/300 - Training Loss: 10.2317, Validation Loss: 9.1309\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/sdk/Work/est_projects/est_wassup_01/test/123.ipynb Cell 4\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/sdk/Work/est_projects/est_wassup_01/test/123.ipynb#W3sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m kfold_cross_validation(ANN, nn\u001b[39m.\u001b[39;49mMSELoss, device, X_trn, y_trn, n_splits \u001b[39m=\u001b[39;49m \u001b[39m5\u001b[39;49m)\n",
      "\u001b[1;32m/home/sdk/Work/est_projects/est_wassup_01/test/123.ipynb Cell 4\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/sdk/Work/est_projects/est_wassup_01/test/123.ipynb#W3sZmlsZQ%3D%3D?line=123'>124</a>\u001b[0m criterion \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mMSELoss(reduction\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmean\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/sdk/Work/est_projects/est_wassup_01/test/123.ipynb#W3sZmlsZQ%3D%3D?line=124'>125</a>\u001b[0m optimizer \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mAdam(net\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39m\u001b[39m0.0001\u001b[39m)\n\u001b[0;32m--> <a href='vscode-notebook-cell:/home/sdk/Work/est_projects/est_wassup_01/test/123.ipynb#W3sZmlsZQ%3D%3D?line=125'>126</a>\u001b[0m loss \u001b[39m=\u001b[39m train_one_epoch(net, criterion, optimizer, dl, device)\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/sdk/Work/est_projects/est_wassup_01/test/123.ipynb#W3sZmlsZQ%3D%3D?line=126'>127</a>\u001b[0m loss_val \u001b[39m=\u001b[39m evaluate(net, criterion, dl_val, device, multi_metrics\u001b[39m=\u001b[39m[mae, mse, msle])\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/sdk/Work/est_projects/est_wassup_01/test/123.ipynb#W3sZmlsZQ%3D%3D?line=127'>128</a>\u001b[0m mae, mse, msle \u001b[39m=\u001b[39m mae\u001b[39m.\u001b[39mcompute(), mse\u001b[39m.\u001b[39mcompute(), msle\u001b[39m.\u001b[39mcompute()\n",
      "\u001b[1;32m/home/sdk/Work/est_projects/est_wassup_01/test/123.ipynb Cell 4\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sdk/Work/est_projects/est_wassup_01/test/123.ipynb#W3sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m X, y \u001b[39m=\u001b[39m X\u001b[39m.\u001b[39mto(device), y\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sdk/Work/est_projects/est_wassup_01/test/123.ipynb#W3sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m output \u001b[39m=\u001b[39m model(X)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/sdk/Work/est_projects/est_wassup_01/test/123.ipynb#W3sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(output, y)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sdk/Work/est_projects/est_wassup_01/test/123.ipynb#W3sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sdk/Work/est_projects/est_wassup_01/test/123.ipynb#W3sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/miniconda3/envs/MathAI/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/MathAI/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/MathAI/lib/python3.10/site-packages/torch/nn/modules/loss.py:535\u001b[0m, in \u001b[0;36mMSELoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    534\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, target: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 535\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mmse_loss(\u001b[39minput\u001b[39;49m, target, reduction\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduction)\n",
      "File \u001b[0;32m~/miniconda3/envs/MathAI/lib/python3.10/site-packages/torch/nn/functional.py:3328\u001b[0m, in \u001b[0;36mmse_loss\u001b[0;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   3325\u001b[0m \u001b[39mif\u001b[39;00m size_average \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m reduce \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   3326\u001b[0m     reduction \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3328\u001b[0m expanded_input, expanded_target \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mbroadcast_tensors(\u001b[39minput\u001b[39;49m, target)\n\u001b[1;32m   3329\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_nn\u001b[39m.\u001b[39mmse_loss(expanded_input, expanded_target, _Reduction\u001b[39m.\u001b[39mget_enum(reduction))\n",
      "File \u001b[0;32m~/miniconda3/envs/MathAI/lib/python3.10/site-packages/torch/functional.py:73\u001b[0m, in \u001b[0;36mbroadcast_tensors\u001b[0;34m(*tensors)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function(tensors):\n\u001b[1;32m     72\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(broadcast_tensors, tensors, \u001b[39m*\u001b[39mtensors)\n\u001b[0;32m---> 73\u001b[0m \u001b[39mreturn\u001b[39;00m _VF\u001b[39m.\u001b[39;49mbroadcast_tensors(tensors)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "kfold_cross_validation(ANN, nn.MSELoss, device, X_trn, y_trn, n_splits = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
